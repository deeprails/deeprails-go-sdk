// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

package deeprails

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"slices"
	"time"

	"github.com/stainless-sdks/deeprails-go/internal/apijson"
	"github.com/stainless-sdks/deeprails-go/internal/requestconfig"
	"github.com/stainless-sdks/deeprails-go/option"
	"github.com/stainless-sdks/deeprails-go/packages/param"
	"github.com/stainless-sdks/deeprails-go/packages/respjson"
)

// EvaluateService contains methods and other services that help with interacting
// with the deeprails API.
//
// Note, unlike clients, this service does not read variables from the environment
// automatically. You should not instantiate this service directly, and instead use
// the [NewEvaluateService] method instead.
type EvaluateService struct {
	Options []option.RequestOption
}

// NewEvaluateService generates a new service that applies the given options to
// each request. These options are applied after the parent client's options (if
// there is one), and before any request-specific options.
func NewEvaluateService(opts ...option.RequestOption) (r EvaluateService) {
	r = EvaluateService{}
	r.Options = opts
	return
}

// Use this endpoint to evaluate a model's input and output pair against selected
// guardrail metrics
func (r *EvaluateService) New(ctx context.Context, body EvaluateNewParams, opts ...option.RequestOption) (res *Evaluation, err error) {
	opts = slices.Concat(r.Options, opts)
	path := "evaluate"
	err = requestconfig.ExecuteNewRequest(ctx, http.MethodPost, path, body, &res, opts...)
	return
}

// Retrieve the evaluation record for a given evaluation ID.
func (r *EvaluateService) Get(ctx context.Context, evalID string, opts ...option.RequestOption) (res *Evaluation, err error) {
	opts = slices.Concat(r.Options, opts)
	if evalID == "" {
		err = errors.New("missing required eval_id parameter")
		return
	}
	path := fmt.Sprintf("evaluate/%s", evalID)
	err = requestconfig.ExecuteNewRequest(ctx, http.MethodGet, path, nil, &res, opts...)
	return
}

type Evaluation struct {
	// A unique evaluation ID.
	EvalID string `json:"eval_id,required"`
	// Status of the evaluation.
	//
	// Any of "in_progress", "completed", "canceled", "queued", "failed".
	EvaluationStatus EvaluationEvaluationStatus `json:"evaluation_status,required"`
	// A dictionary of inputs sent to the LLM to generate output. The dictionary must
	// contain a `user_prompt` field and an optional `context` field. Additional
	// properties are allowed.
	ModelInput EvaluationModelInput `json:"model_input,required"`
	// Output generated by the LLM to be evaluated.
	ModelOutput string `json:"model_output,required"`
	// Run mode for the evaluation. The run mode allows the user to optimize for speed,
	// accuracy, and cost by determining which models are used to evaluate the event.
	//
	// Any of "precision_plus", "precision", "smart", "economy".
	RunMode EvaluationRunMode `json:"run_mode,required"`
	// The time the evaluation was created in UTC.
	CreatedAt time.Time `json:"created_at" format:"date-time"`
	// The time the evaluation completed in UTC.
	EndTimestamp time.Time `json:"end_timestamp" format:"date-time"`
	// Description of the error causing the evaluation to fail, if any.
	ErrorMessage string `json:"error_message"`
	// The time the error causing the evaluation to fail was recorded.
	ErrorTimestamp time.Time `json:"error_timestamp" format:"date-time"`
	// Evaluation result consisting of average scores and rationales for each of the
	// evaluated guardrail metrics.
	EvaluationResult map[string]any `json:"evaluation_result"`
	// Total cost of the evaluation.
	EvaluationTotalCost float64 `json:"evaluation_total_cost"`
	// An array of guardrail metrics that the model input and output pair will be
	// evaluated on.
	//
	// Any of "correctness", "completeness", "instruction_adherence",
	// "context_adherence", "ground_truth_adherence", "comprehensive_safety".
	GuardrailMetrics []string `json:"guardrail_metrics"`
	// Model ID used to generate the output, like `gpt-4o` or `o3`.
	ModelUsed string `json:"model_used"`
	// The most recent time the evaluation was modified in UTC.
	ModifiedAt time.Time `json:"modified_at" format:"date-time"`
	// An optional, user-defined tag for the evaluation.
	Nametag string `json:"nametag"`
	// Evaluation progress. Values range between 0 and 100; 100 corresponds to a
	// completed `evaluation_status`.
	Progress int64 `json:"progress"`
	// The time the evaluation started in UTC.
	StartTimestamp time.Time `json:"start_timestamp" format:"date-time"`
	// JSON contains metadata for fields, check presence with [respjson.Field.Valid].
	JSON struct {
		EvalID              respjson.Field
		EvaluationStatus    respjson.Field
		ModelInput          respjson.Field
		ModelOutput         respjson.Field
		RunMode             respjson.Field
		CreatedAt           respjson.Field
		EndTimestamp        respjson.Field
		ErrorMessage        respjson.Field
		ErrorTimestamp      respjson.Field
		EvaluationResult    respjson.Field
		EvaluationTotalCost respjson.Field
		GuardrailMetrics    respjson.Field
		ModelUsed           respjson.Field
		ModifiedAt          respjson.Field
		Nametag             respjson.Field
		Progress            respjson.Field
		StartTimestamp      respjson.Field
		ExtraFields         map[string]respjson.Field
		raw                 string
	} `json:"-"`
}

// Returns the unmodified JSON received from the API
func (r Evaluation) RawJSON() string { return r.JSON.raw }
func (r *Evaluation) UnmarshalJSON(data []byte) error {
	return apijson.UnmarshalRoot(data, r)
}

// Status of the evaluation.
type EvaluationEvaluationStatus string

const (
	EvaluationEvaluationStatusInProgress EvaluationEvaluationStatus = "in_progress"
	EvaluationEvaluationStatusCompleted  EvaluationEvaluationStatus = "completed"
	EvaluationEvaluationStatusCanceled   EvaluationEvaluationStatus = "canceled"
	EvaluationEvaluationStatusQueued     EvaluationEvaluationStatus = "queued"
	EvaluationEvaluationStatusFailed     EvaluationEvaluationStatus = "failed"
)

// A dictionary of inputs sent to the LLM to generate output. The dictionary must
// contain a `user_prompt` field and an optional `context` field. Additional
// properties are allowed.
type EvaluationModelInput struct {
	// The user prompt used to generate the output.
	UserPrompt string `json:"user_prompt,required"`
	// Optional context supplied to the LLM when generating the output.
	Context     string         `json:"context"`
	ExtraFields map[string]any `json:",extras"`
	// JSON contains metadata for fields, check presence with [respjson.Field.Valid].
	JSON struct {
		UserPrompt  respjson.Field
		Context     respjson.Field
		ExtraFields map[string]respjson.Field
		raw         string
	} `json:"-"`
}

// Returns the unmodified JSON received from the API
func (r EvaluationModelInput) RawJSON() string { return r.JSON.raw }
func (r *EvaluationModelInput) UnmarshalJSON(data []byte) error {
	return apijson.UnmarshalRoot(data, r)
}

// Run mode for the evaluation. The run mode allows the user to optimize for speed,
// accuracy, and cost by determining which models are used to evaluate the event.
type EvaluationRunMode string

const (
	EvaluationRunModePrecisionPlus EvaluationRunMode = "precision_plus"
	EvaluationRunModePrecision     EvaluationRunMode = "precision"
	EvaluationRunModeSmart         EvaluationRunMode = "smart"
	EvaluationRunModeEconomy       EvaluationRunMode = "economy"
)

type EvaluateNewParams struct {
	// A dictionary of inputs sent to the LLM to generate output. This must contain a
	// `user_prompt` field and an optional `context` field. Additional properties are
	// allowed.
	ModelInput EvaluateNewParamsModelInput `json:"model_input,omitzero,required"`
	// Output generated by the LLM to be evaluated.
	ModelOutput string `json:"model_output,required"`
	// Run mode for the evaluation. The run mode allows the user to optimize for speed,
	// accuracy, and cost by determining which models are used to evaluate the event.
	// Available run modes include `precision_plus`, `precision`, `smart`, and
	// `economy`. Defaults to `smart`.
	//
	// Any of "precision_plus", "precision", "smart", "economy".
	RunMode EvaluateNewParamsRunMode `json:"run_mode,omitzero,required"`
	// Model ID used to generate the output, like `gpt-4o` or `o3`.
	ModelUsed param.Opt[string] `json:"model_used,omitzero"`
	// An optional, user-defined tag for the evaluation.
	Nametag param.Opt[string] `json:"nametag,omitzero"`
	// An array of guardrail metrics that the model input and output pair will be
	// evaluated on. For non-enterprise users, these will be limited to the allowed
	// guardrail metrics.
	//
	// Any of "correctness", "completeness", "instruction_adherence",
	// "context_adherence", "ground_truth_adherence", "comprehensive_safety".
	GuardrailMetrics []string `json:"guardrail_metrics,omitzero"`
	paramObj
}

func (r EvaluateNewParams) MarshalJSON() (data []byte, err error) {
	type shadow EvaluateNewParams
	return param.MarshalObject(r, (*shadow)(&r))
}
func (r *EvaluateNewParams) UnmarshalJSON(data []byte) error {
	return apijson.UnmarshalRoot(data, r)
}

// A dictionary of inputs sent to the LLM to generate output. This must contain a
// `user_prompt` field and an optional `context` field. Additional properties are
// allowed.
//
// The property UserPrompt is required.
type EvaluateNewParamsModelInput struct {
	UserPrompt  string            `json:"user_prompt,required"`
	Context     param.Opt[string] `json:"context,omitzero"`
	ExtraFields map[string]any    `json:"-"`
	paramObj
}

func (r EvaluateNewParamsModelInput) MarshalJSON() (data []byte, err error) {
	type shadow EvaluateNewParamsModelInput
	return param.MarshalWithExtras(r, (*shadow)(&r), r.ExtraFields)
}
func (r *EvaluateNewParamsModelInput) UnmarshalJSON(data []byte) error {
	return apijson.UnmarshalRoot(data, r)
}

// Run mode for the evaluation. The run mode allows the user to optimize for speed,
// accuracy, and cost by determining which models are used to evaluate the event.
// Available run modes include `precision_plus`, `precision`, `smart`, and
// `economy`. Defaults to `smart`.
type EvaluateNewParamsRunMode string

const (
	EvaluateNewParamsRunModePrecisionPlus EvaluateNewParamsRunMode = "precision_plus"
	EvaluateNewParamsRunModePrecision     EvaluateNewParamsRunMode = "precision"
	EvaluateNewParamsRunModeSmart         EvaluateNewParamsRunMode = "smart"
	EvaluateNewParamsRunModeEconomy       EvaluateNewParamsRunMode = "economy"
)
