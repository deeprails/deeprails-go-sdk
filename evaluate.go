// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

package deeprails

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"slices"
	"time"

	"github.com/deeprails/deeprails-go-sdk/internal/apijson"
	"github.com/deeprails/deeprails-go-sdk/internal/param"
	"github.com/deeprails/deeprails-go-sdk/internal/requestconfig"
	"github.com/deeprails/deeprails-go-sdk/option"
)

// EvaluateService contains methods and other services that help with interacting
// with the deeprails API.
//
// Note, unlike clients, this service does not read variables from the environment
// automatically. You should not instantiate this service directly, and instead use
// the [NewEvaluateService] method instead.
type EvaluateService struct {
	Options []option.RequestOption
}

// NewEvaluateService generates a new service that applies the given options to
// each request. These options are applied after the parent client's options (if
// there is one), and before any request-specific options.
func NewEvaluateService(opts ...option.RequestOption) (r *EvaluateService) {
	r = &EvaluateService{}
	r.Options = opts
	return
}

// Use this endpoint to evaluate a model's input and output pair against selected
// guardrail metrics
func (r *EvaluateService) New(ctx context.Context, body EvaluateNewParams, opts ...option.RequestOption) (res *Evaluation, err error) {
	opts = slices.Concat(r.Options, opts)
	path := "evaluate"
	err = requestconfig.ExecuteNewRequest(ctx, http.MethodPost, path, body, &res, opts...)
	return
}

// Use this endpoint to retrieve the evaluation record for a given evaluation ID
func (r *EvaluateService) Get(ctx context.Context, evalID string, opts ...option.RequestOption) (res *Evaluation, err error) {
	opts = slices.Concat(r.Options, opts)
	if evalID == "" {
		err = errors.New("missing required eval_id parameter")
		return
	}
	path := fmt.Sprintf("evaluate/%s", evalID)
	err = requestconfig.ExecuteNewRequest(ctx, http.MethodGet, path, nil, &res, opts...)
	return
}

type Evaluation struct {
	// A unique evaluation ID.
	EvalID string `json:"eval_id,required"`
	// Status of the evaluation.
	EvaluationStatus EvaluationEvaluationStatus `json:"evaluation_status,required"`
	// A dictionary of inputs sent to the LLM to generate output. The dictionary must
	// contain at least `user_prompt` or `system_prompt` field. For
	// ground_truth_aherence guadrail metric, `ground_truth` should be provided.
	ModelInput EvaluationModelInput `json:"model_input,required"`
	// Output generated by the LLM to be evaluated.
	ModelOutput string `json:"model_output,required"`
	// Run mode for the evaluation. The run mode allows the user to optimize for speed,
	// accuracy, and cost by determining which models are used to evaluate the event.
	RunMode EvaluationRunMode `json:"run_mode,required"`
	// The time the evaluation was created in UTC.
	CreatedAt time.Time `json:"created_at" format:"date-time"`
	// The time the evaluation completed in UTC.
	EndTimestamp time.Time `json:"end_timestamp" format:"date-time"`
	// Description of the error causing the evaluation to fail, if any.
	ErrorMessage string `json:"error_message"`
	// The time the error causing the evaluation to fail was recorded.
	ErrorTimestamp time.Time `json:"error_timestamp" format:"date-time"`
	// Evaluation result consisting of average scores and rationales for each of the
	// evaluated guardrail metrics.
	EvaluationResult map[string]interface{} `json:"evaluation_result"`
	// Total cost of the evaluation.
	EvaluationTotalCost float64 `json:"evaluation_total_cost"`
	// An array of guardrail metrics that the model input and output pair will be
	// evaluated on.
	GuardrailMetrics []EvaluationGuardrailMetric `json:"guardrail_metrics"`
	// Model ID used to generate the output, like `gpt-4o` or `o3`.
	ModelUsed string `json:"model_used"`
	// The most recent time the evaluation was modified in UTC.
	ModifiedAt time.Time `json:"modified_at" format:"date-time"`
	// An optional, user-defined tag for the evaluation.
	Nametag string `json:"nametag"`
	// Evaluation progress. Values range between 0 and 100; 100 corresponds to a
	// completed `evaluation_status`.
	Progress int64 `json:"progress"`
	// The time the evaluation started in UTC.
	StartTimestamp time.Time      `json:"start_timestamp" format:"date-time"`
	JSON           evaluationJSON `json:"-"`
}

// evaluationJSON contains the JSON metadata for the struct [Evaluation]
type evaluationJSON struct {
	EvalID              apijson.Field
	EvaluationStatus    apijson.Field
	ModelInput          apijson.Field
	ModelOutput         apijson.Field
	RunMode             apijson.Field
	CreatedAt           apijson.Field
	EndTimestamp        apijson.Field
	ErrorMessage        apijson.Field
	ErrorTimestamp      apijson.Field
	EvaluationResult    apijson.Field
	EvaluationTotalCost apijson.Field
	GuardrailMetrics    apijson.Field
	ModelUsed           apijson.Field
	ModifiedAt          apijson.Field
	Nametag             apijson.Field
	Progress            apijson.Field
	StartTimestamp      apijson.Field
	raw                 string
	ExtraFields         map[string]apijson.Field
}

func (r *Evaluation) UnmarshalJSON(data []byte) (err error) {
	return apijson.UnmarshalRoot(data, r)
}

func (r evaluationJSON) RawJSON() string {
	return r.raw
}

// Status of the evaluation.
type EvaluationEvaluationStatus string

const (
	EvaluationEvaluationStatusInProgress EvaluationEvaluationStatus = "in_progress"
	EvaluationEvaluationStatusCompleted  EvaluationEvaluationStatus = "completed"
	EvaluationEvaluationStatusCanceled   EvaluationEvaluationStatus = "canceled"
	EvaluationEvaluationStatusQueued     EvaluationEvaluationStatus = "queued"
	EvaluationEvaluationStatusFailed     EvaluationEvaluationStatus = "failed"
)

func (r EvaluationEvaluationStatus) IsKnown() bool {
	switch r {
	case EvaluationEvaluationStatusInProgress, EvaluationEvaluationStatusCompleted, EvaluationEvaluationStatusCanceled, EvaluationEvaluationStatusQueued, EvaluationEvaluationStatusFailed:
		return true
	}
	return false
}

// A dictionary of inputs sent to the LLM to generate output. The dictionary must
// contain at least `user_prompt` or `system_prompt` field. For
// ground_truth_aherence guadrail metric, `ground_truth` should be provided.
type EvaluationModelInput struct {
	// The ground truth for evaluating Ground Truth Adherence guardrail.
	GroundTruth string `json:"ground_truth"`
	// The system prompt used to generate the output.
	SystemPrompt string `json:"system_prompt"`
	// The user prompt used to generate the output.
	UserPrompt string                   `json:"user_prompt"`
	JSON       evaluationModelInputJSON `json:"-"`
}

// evaluationModelInputJSON contains the JSON metadata for the struct
// [EvaluationModelInput]
type evaluationModelInputJSON struct {
	GroundTruth  apijson.Field
	SystemPrompt apijson.Field
	UserPrompt   apijson.Field
	raw          string
	ExtraFields  map[string]apijson.Field
}

func (r *EvaluationModelInput) UnmarshalJSON(data []byte) (err error) {
	return apijson.UnmarshalRoot(data, r)
}

func (r evaluationModelInputJSON) RawJSON() string {
	return r.raw
}

// Run mode for the evaluation. The run mode allows the user to optimize for speed,
// accuracy, and cost by determining which models are used to evaluate the event.
type EvaluationRunMode string

const (
	EvaluationRunModePrecisionPlus EvaluationRunMode = "precision_plus"
	EvaluationRunModePrecision     EvaluationRunMode = "precision"
	EvaluationRunModeSmart         EvaluationRunMode = "smart"
	EvaluationRunModeEconomy       EvaluationRunMode = "economy"
)

func (r EvaluationRunMode) IsKnown() bool {
	switch r {
	case EvaluationRunModePrecisionPlus, EvaluationRunModePrecision, EvaluationRunModeSmart, EvaluationRunModeEconomy:
		return true
	}
	return false
}

type EvaluationGuardrailMetric string

const (
	EvaluationGuardrailMetricCorrectness          EvaluationGuardrailMetric = "correctness"
	EvaluationGuardrailMetricCompleteness         EvaluationGuardrailMetric = "completeness"
	EvaluationGuardrailMetricInstructionAdherence EvaluationGuardrailMetric = "instruction_adherence"
	EvaluationGuardrailMetricContextAdherence     EvaluationGuardrailMetric = "context_adherence"
	EvaluationGuardrailMetricGroundTruthAdherence EvaluationGuardrailMetric = "ground_truth_adherence"
	EvaluationGuardrailMetricComprehensiveSafety  EvaluationGuardrailMetric = "comprehensive_safety"
)

func (r EvaluationGuardrailMetric) IsKnown() bool {
	switch r {
	case EvaluationGuardrailMetricCorrectness, EvaluationGuardrailMetricCompleteness, EvaluationGuardrailMetricInstructionAdherence, EvaluationGuardrailMetricContextAdherence, EvaluationGuardrailMetricGroundTruthAdherence, EvaluationGuardrailMetricComprehensiveSafety:
		return true
	}
	return false
}

type EvaluateNewParams struct {
	// A dictionary of inputs sent to the LLM to generate output. The dictionary must
	// contain at least `user_prompt` or `system_prompt` field. For
	// ground_truth_aherence guadrail metric, `ground_truth` should be provided.
	ModelInput param.Field[EvaluateNewParamsModelInput] `json:"model_input,required"`
	// Output generated by the LLM to be evaluated.
	ModelOutput param.Field[string] `json:"model_output,required"`
	// Run mode for the evaluation. The run mode allows the user to optimize for speed,
	// accuracy, and cost by determining which models are used to evaluate the event.
	// Available run modes include `precision_plus`, `precision`, `smart`, and
	// `economy`. Defaults to `smart`.
	RunMode param.Field[EvaluateNewParamsRunMode] `json:"run_mode,required"`
	// An array of guardrail metrics that the model input and output pair will be
	// evaluated on. For non-enterprise users, these will be limited to the allowed
	// guardrail metrics.
	GuardrailMetrics param.Field[[]EvaluateNewParamsGuardrailMetric] `json:"guardrail_metrics"`
	// Model ID used to generate the output, like `gpt-4o` or `o3`.
	ModelUsed param.Field[string] `json:"model_used"`
	// An optional, user-defined tag for the evaluation.
	Nametag param.Field[string] `json:"nametag"`
}

func (r EvaluateNewParams) MarshalJSON() (data []byte, err error) {
	return apijson.MarshalRoot(r)
}

// A dictionary of inputs sent to the LLM to generate output. The dictionary must
// contain at least `user_prompt` or `system_prompt` field. For
// ground_truth_aherence guadrail metric, `ground_truth` should be provided.
type EvaluateNewParamsModelInput struct {
	// The ground truth for evaluating Ground Truth Adherence guardrail.
	GroundTruth param.Field[string] `json:"ground_truth"`
	// The system prompt used to generate the output.
	SystemPrompt param.Field[string] `json:"system_prompt"`
	// The user prompt used to generate the output.
	UserPrompt param.Field[string] `json:"user_prompt"`
}

func (r EvaluateNewParamsModelInput) MarshalJSON() (data []byte, err error) {
	return apijson.MarshalRoot(r)
}

// Run mode for the evaluation. The run mode allows the user to optimize for speed,
// accuracy, and cost by determining which models are used to evaluate the event.
// Available run modes include `precision_plus`, `precision`, `smart`, and
// `economy`. Defaults to `smart`.
type EvaluateNewParamsRunMode string

const (
	EvaluateNewParamsRunModePrecisionPlus EvaluateNewParamsRunMode = "precision_plus"
	EvaluateNewParamsRunModePrecision     EvaluateNewParamsRunMode = "precision"
	EvaluateNewParamsRunModeSmart         EvaluateNewParamsRunMode = "smart"
	EvaluateNewParamsRunModeEconomy       EvaluateNewParamsRunMode = "economy"
)

func (r EvaluateNewParamsRunMode) IsKnown() bool {
	switch r {
	case EvaluateNewParamsRunModePrecisionPlus, EvaluateNewParamsRunModePrecision, EvaluateNewParamsRunModeSmart, EvaluateNewParamsRunModeEconomy:
		return true
	}
	return false
}

type EvaluateNewParamsGuardrailMetric string

const (
	EvaluateNewParamsGuardrailMetricCorrectness          EvaluateNewParamsGuardrailMetric = "correctness"
	EvaluateNewParamsGuardrailMetricCompleteness         EvaluateNewParamsGuardrailMetric = "completeness"
	EvaluateNewParamsGuardrailMetricInstructionAdherence EvaluateNewParamsGuardrailMetric = "instruction_adherence"
	EvaluateNewParamsGuardrailMetricContextAdherence     EvaluateNewParamsGuardrailMetric = "context_adherence"
	EvaluateNewParamsGuardrailMetricGroundTruthAdherence EvaluateNewParamsGuardrailMetric = "ground_truth_adherence"
	EvaluateNewParamsGuardrailMetricComprehensiveSafety  EvaluateNewParamsGuardrailMetric = "comprehensive_safety"
)

func (r EvaluateNewParamsGuardrailMetric) IsKnown() bool {
	switch r {
	case EvaluateNewParamsGuardrailMetricCorrectness, EvaluateNewParamsGuardrailMetricCompleteness, EvaluateNewParamsGuardrailMetricInstructionAdherence, EvaluateNewParamsGuardrailMetricContextAdherence, EvaluateNewParamsGuardrailMetricGroundTruthAdherence, EvaluateNewParamsGuardrailMetricComprehensiveSafety:
		return true
	}
	return false
}
